#!/usr/bin/env python3


import sys, os
import numpy as np
import torch
from torch import nn
import torch.optim as optim
from torch.optim import lr_scheduler
import time
from time import perf_counter
import pickle
import multiprocessing as mp
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor
import threading
from model.config import load_config
from model.genconvit_ed import GenConViTED
from model.genconvit_vae import GenConViTVAE
from dataset.loader import load_data, load_checkpoint
import argparse

config = load_config()

# RTX 5090 GPU ìµœì í™” ì„¤ì • (í˜¸í™˜ì„± ë¬¸ì œë¡œ CPU ì‚¬ìš©)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
if device.type == "cpu":
    print("âš ï¸  RTX 5090 í˜¸í™˜ì„± ë¬¸ì œë¡œ CPUë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
    print("ğŸ’¡ GPU ì‚¬ìš©ì„ ì›í•œë‹¤ë©´ ìµœì‹  PyTorchë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”.")
    print("="*60)

def train_single_model_parallel(model_config):
    """ë‹¨ì¼ ëª¨ë¸ì„ ë³‘ë ¬ë¡œ í•™ìŠµí•˜ëŠ” í•¨ìˆ˜"""
    model_type, dataloaders, num_epochs, pretrained_model_filename, batch_size, model_name, device_id = model_config
    
    # ê° í”„ë¡œì„¸ìŠ¤ë§ˆë‹¤ ë‹¤ë¥¸ GPU ì‚¬ìš© (GPUê°€ ì—¬ëŸ¬ ê°œì¸ ê²½ìš°)
    if torch.cuda.is_available() and torch.cuda.device_count() > 1:
        device = torch.device(f"cuda:{device_id % torch.cuda.device_count()}")
    else:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    print(f"\n{'='*60}")
    print(f"ğŸ¤– {model_name} ëª¨ë¸ í•™ìŠµ ì‹œì‘ (Device: {device})")
    print(f"{'='*60}")
    
    # ëª¨ë¸ ìƒì„±
    if model_type == "ed":
        model = GenConViTED(config)
    else:
        model = GenConViTVAE(config)
    
    # íŒŒì¸íŠœë‹ì„ ìœ„í•œ ë‚®ì€ í•™ìŠµë¥  ì„¤ì •
    if pretrained_model_filename:
        learning_rate = float(config["learning_rate"]) * 0.1
        print(f"ğŸ”§ íŒŒì¸íŠœë‹ ëª¨ë“œ: í•™ìŠµë¥  {learning_rate:.6f} (ê¸°ë³¸ê°’ì˜ 10%)")
    else:
        learning_rate = float(config["learning_rate"])
        print(f"ğŸ”§ ì²˜ìŒë¶€í„° í•™ìŠµ: í•™ìŠµë¥  {learning_rate:.6f}")
    
    optimizer = optim.Adam(
        model.parameters(),
        lr=learning_rate,
        weight_decay=float(config["weight_decay"]),
    )
    
    # í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ê²°ì„ ìœ„í•œ ê°€ì¤‘ì¹˜ ì„¤ì • (Real:Fake â‰ˆ 1:5.27)
    # Real í´ë˜ìŠ¤(0)ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬
    weights = torch.tensor([5.27, 1.0]).to(device)
    criterion = nn.CrossEntropyLoss(weight=weights)
    criterion.to(device)
    mse = nn.MSELoss()
    
    # ê²€ì¦ ì†ì‹¤ ê¸°ë°˜ìœ¼ë¡œ í•™ìŠµë¥  ë™ì  ì¡°ì • (verbose ì¸ì ì œê±°)
    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.1)
    
    if pretrained_model_filename:
        print(f"ğŸ”„ ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ ë¡œë”©: {pretrained_model_filename}")
        model, optimizer, start_epoch, min_loss = load_pretrained(
            model, optimizer, pretrained_model_filename
        )
    else:
        start_epoch = 0
        min_loss = float('inf')
    
    model.to(device)
    torch.manual_seed(1)
    train_loss, train_acc, valid_loss, valid_acc = [], [], [], []
    since = time.time()
    
    # Early Stopping ì„¤ì •
    patience = 10
    best_epoch = 0
    no_improve_count = 0
    
    print(f"\ní•™ìŠµ ì„¤ì •:")
    print(f"  ì´ ì—í¬í¬: {num_epochs}")
    print(f"  Early Stopping: {patience} ì—í¬í¬ ë™ì•ˆ ê°œì„  ì—†ìœ¼ë©´ ì¤‘ë‹¨")
    print(f"  ë°°ì¹˜ í¬ê¸°: {batch_size}")
    print(f"  ëª©í‘œ: ê²€ì¦ ì†ì‹¤ ìµœì†Œí™”")
    print("="*60)
    
    for epoch in range(start_epoch, num_epochs):
        epoch_start_time = time.time()
        
        # í•™ìŠµ í•¨ìˆ˜ import
        if model_type == "ed":
            from train.train_ed import train, valid
        else:
            from train.train_vae import train, valid
        
        # í•™ìŠµ
        train_loss, train_acc, epoch_train_loss = train(
            model,
            device,
            dataloaders["train"],
            criterion,
            optimizer,
            epoch,
            train_loss,
            train_acc,
            mse,
        )
        
        # ê²€ì¦
        valid_loss, valid_acc = valid(
            model,
            device,
            dataloaders["valid"],
            criterion,
            epoch,
            valid_loss,
            valid_acc,
            mse,
        )
        
        # ìŠ¤ì¼€ì¤„ëŸ¬ step í˜¸ì¶œ (ReduceLROnPlateauëŠ” validation lossë¥¼ ì¸ìë¡œ ë°›ìŒ)
        scheduler.step(valid_loss[-1])
        
        # ì‹œê°ì  ë¡œê¹…
        epoch_time = time.time() - epoch_start_time
        current_lr = optimizer.param_groups[0]['lr']
        
        # ì§„í–‰ë¥  ë°” ê³„ì‚°
        progress = (epoch + 1) / num_epochs
        bar_length = 30
        filled_length = int(bar_length * progress)
        bar = 'â–ˆ' * filled_length + 'â–‘' * (bar_length - filled_length)
        
        # ì„±ëŠ¥ ê°œì„  í‘œì‹œ
        if epoch > 0:
            loss_improvement = valid_loss[-2] - valid_loss[-1] if len(valid_loss) > 1 else 0
            acc_improvement = valid_acc[-1] - valid_acc[-2] if len(valid_acc) > 1 else 0
            
            loss_arrow = "â†‘" if loss_improvement > 0 else "â†“" if loss_improvement < 0 else "â†’"
            acc_arrow = "â†‘" if acc_improvement > 0 else "â†“" if acc_improvement < 0 else "â†’"
        else:
            loss_arrow = "NEW"
            acc_arrow = "NEW"
        
        print(f"\n[{model_name}] EPOCH {epoch+1:2d}/{num_epochs} | {bar} | {progress*100:5.1f}%")
        print(f"[{model_name}] ì‹œê°„: {epoch_time:6.2f}ì´ˆ | í•™ìŠµë¥ : {current_lr:.2e}")
        print(f"[{model_name}] í•™ìŠµ ì†ì‹¤: {epoch_train_loss:8.4f} | {loss_arrow} ê²€ì¦ ì†ì‹¤: {valid_loss[-1]:8.4f}")
        print(f"[{model_name}] ê²€ì¦ ì •í™•ë„: {valid_acc[-1]*100:6.2f}% | {acc_arrow} ê°œì„ : {acc_improvement*100:+.2f}%" if epoch > 0 else f"[{model_name}] ê²€ì¦ ì •í™•ë„: {valid_acc[-1]*100:6.2f}% | {acc_arrow}")
        print("-" * 60)
        
        # Early Stopping ë° ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
        if valid_loss[-1] < min_loss:
            min_loss = valid_loss[-1]
            best_epoch = epoch
            no_improve_count = 0
            best_model_path = os.path.join("weight", f"best_genconvit_{model_type}.pth")
            torch.save({
                "epoch": epoch,
                "state_dict": model.state_dict(),
                "optimizer": optimizer.state_dict(),
                "min_loss": min_loss,
                "valid_acc": valid_acc[-1],
                "model_type": model_type
            }, best_model_path)
            print(f"[{model_name}] â˜… ìƒˆë¡œìš´ ìµœê³  ì„±ëŠ¥! ëª¨ë¸ ì €ì¥: {best_model_path}")
        else:
            no_improve_count += 1
            print(f"[{model_name}] âš ï¸  {no_improve_count}/{patience} ì—í¬í¬ ë™ì•ˆ ê°œì„  ì—†ìŒ")
            
            # Early Stopping ì²´í¬
            if no_improve_count >= patience:
                print(f"\n[{model_name}] ğŸ›‘ Early Stopping! {patience} ì—í¬í¬ ë™ì•ˆ ê°œì„  ì—†ì–´ì„œ í•™ìŠµ ì¤‘ë‹¨")
                print(f"[{model_name}] â˜… ìµœê³  ì„±ëŠ¥: ì—í¬í¬ {best_epoch}, ì†ì‹¤ {min_loss:.4f}")
                break
    
    time_elapsed = time.time() - since
    
    print(f"\n{'='*60}")
    print(f"âœ“ {model_name} í•™ìŠµ ì™„ë£Œ!")
    print(f"{'='*60}")
    print(f"ì´ ì†Œìš” ì‹œê°„: {time_elapsed // 60:.0f}ë¶„ {time_elapsed % 60:.0f}ì´ˆ")
    print(f"ìµœì¢… ê²€ì¦ ì •í™•ë„: {valid_acc[-1]*100:.2f}%")
    print(f"ìµœì¢… ê²€ì¦ ì†ì‹¤: {valid_loss[-1]:.4f}")
    print(f"ìµœê³  ì„±ëŠ¥: ì—í¬í¬ {best_epoch}, ì†ì‹¤ {min_loss:.4f}")
    print(f"{'='*60}")
    
    # ëª¨ë¸ ì €ì¥
    file_path = os.path.join(
        "weight",
        f'genconvit_{model_type}_{time.strftime("%b_%d_%Y_%H_%M_%S", time.localtime())}',
    )
    
    # í•™ìŠµ íˆìŠ¤í† ë¦¬ ì €ì¥
    with open(f"{file_path}.pkl", "wb") as f:
        pickle.dump([train_loss, train_acc, valid_loss, valid_acc], f)
    
    # ìµœì¢… ëª¨ë¸ ì €ì¥
    state = {
        "epoch": num_epochs,
        "state_dict": model.state_dict(),
        "optimizer": optimizer.state_dict(),
        "min_loss": valid_loss[-1],
        "final_acc": valid_acc[-1],
        "config": config,
        "model_type": model_type
    }
    
    weight = f"{file_path}.pth"
    torch.save(state, weight)
    
    print(f"[{model_name}] ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {weight}")
    print("="*60)
    
    return {
        "model_type": model_type,
        "model_name": model_name,
        "weight_path": weight,
        "final_acc": valid_acc[-1],
        "final_loss": valid_loss[-1],
        "best_epoch": best_epoch,
        "min_loss": min_loss,
        "training_time": time_elapsed
    }


def train_parallel_models(data_path, num_epochs, batch_size, pretrained_ed=None, pretrained_vae=None):
    """ë‘ ëª¨ë¸(ED, VAE)ì„ ë³‘ë ¬ë¡œ í•™ìŠµ"""
    print(f"\n{'='*80}")
    print("ğŸš€ GenConViT ë³‘ë ¬ í•™ìŠµ ì‹œì‘!")
    print(f"{'='*80}")
    print(f"ë°ì´í„° ê²½ë¡œ: {data_path}")
    print(f"ì—í¬í¬ ìˆ˜: {num_epochs}")
    print(f"ë°°ì¹˜ í¬ê¸°: {batch_size}")
    print(f"ì‚¬ì „ í›ˆë ¨ ED ëª¨ë¸: {pretrained_ed}")
    print(f"ì‚¬ì „ í›ˆë ¨ VAE ëª¨ë¸: {pretrained_vae}")
    print(f"{'='*80}")
    
    start_time = perf_counter()
    
    # ë°ì´í„° ë¡œë”©
    print("ë°ì´í„° ë¡œë”© ì¤‘...")
    dataloaders, dataset_sizes = load_data(data_path, batch_size)
    print("ë°ì´í„° ë¡œë”© ì™„ë£Œ!")
    
    print(f"ë°ì´í„°ì…‹ í¬ê¸°:")
    for split, size in dataset_sizes.items():
        print(f"  {split}: {size:,}ê°œ")
    
    # ë³‘ë ¬ í•™ìŠµì„ ìœ„í•œ ì„¤ì •
    model_configs = [
        ("ed", dataloaders, num_epochs, pretrained_ed, batch_size, "ED (Autoencoder)", 0),
        ("vae", dataloaders, num_epochs, pretrained_vae, batch_size, "VAE (Variational Autoencoder)", 1)
    ]
    
    # ë³‘ë ¬ í•™ìŠµ ì‹¤í–‰
    print(f"\nğŸ”„ ë‘ ëª¨ë¸ì„ ë³‘ë ¬ë¡œ í•™ìŠµ ì‹œì‘...")
    
    # ThreadPoolExecutorë¥¼ ì‚¬ìš©í•˜ì—¬ ë³‘ë ¬ ì‹¤í–‰ (GPU ë©”ëª¨ë¦¬ ê³µìœ ë¥¼ ìœ„í•´)
    with ThreadPoolExecutor(max_workers=2) as executor:
        futures = [executor.submit(train_single_model_parallel, config) for config in model_configs]
        results = [future.result() for future in futures]
    
    end_time = perf_counter()
    
    # ê²°ê³¼ ì¢…í•©
    print(f"\n{'='*80}")
    print("ğŸ‰ ë³‘ë ¬ í•™ìŠµ ì™„ë£Œ!")
    print(f"{'='*80}")
    print(f"ì „ì²´ ì‹¤í–‰ ì‹œê°„: {(end_time - start_time) / 60:.1f}ë¶„")
    
    print(f"\nğŸ“Š ìµœì¢… ê²°ê³¼ ë¹„êµ:")
    print(f"{'='*80}")
    print(f"{'ëª¨ë¸':<20} {'ì •í™•ë„':<10} {'ì†ì‹¤':<10} {'ìµœê³  ì—í¬í¬':<12} {'í•™ìŠµì‹œê°„':<10}")
    print(f"{'-'*80}")
    
    for result in results:
        print(f"{result['model_name']:<20} {result['final_acc']*100:>8.2f}% {result['final_loss']:>8.4f} {result['best_epoch']:>10} {result['training_time']/60:>8.1f}ë¶„")
    
    # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì°¾ê¸°
    best_model = max(results, key=lambda x: x['final_acc'])
    print(f"\nğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model['model_name']}")
    print(f"   ì •í™•ë„: {best_model['final_acc']*100:.2f}%")
    print(f"   ì†ì‹¤: {best_model['final_loss']:.4f}")
    print(f"   ëª¨ë¸ íŒŒì¼: {best_model['weight_path']}")
    print(f"{'='*80}")
    
    return results


def load_pretrained(model, optimizer, pretrained_model_filename):
    """ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ ë¡œë“œ"""
    assert os.path.isfile(
        pretrained_model_filename
    ), "Saved model file does not exist. Exiting."

    model, optimizer, start_epoch, min_loss = load_checkpoint(
        model, optimizer, filename=pretrained_model_filename
    )
    # optimizer ìƒíƒœë¥¼ GPUë¡œ ì´ë™
    for state in optimizer.state.values():
        for k, v in state.items():
            if isinstance(v, torch.Tensor):
                state[k] = v.to(device)
    return model, optimizer, start_epoch, min_loss


def train_simple_model(
    model, mod, dataloaders, num_epochs, pretrained_model_filename, batch_size, model_name
):
    """WandB ì—†ì´ ê°„ë‹¨í•œ ëª¨ë¸ í•™ìŠµ"""
    print(f"\n{'='*60}")
    print(f"ğŸ¤– {model_name} ëª¨ë¸ í•™ìŠµ ì‹œì‘")
    print(f"{'='*60}")

    # íŒŒì¸íŠœë‹ì„ ìœ„í•œ ë‚®ì€ í•™ìŠµë¥  ì„¤ì •
    if pretrained_model_filename:
        learning_rate = float(config["learning_rate"]) * 0.1
        print(f"ğŸ”§ íŒŒì¸íŠœë‹ ëª¨ë“œ: í•™ìŠµë¥  {learning_rate:.6f} (ê¸°ë³¸ê°’ì˜ 10%)")
    else:
        learning_rate = float(config["learning_rate"])
        print(f"ğŸ”§ ì²˜ìŒë¶€í„° í•™ìŠµ: í•™ìŠµë¥  {learning_rate:.6f}")

    optimizer = optim.Adam(
        model.parameters(),
        lr=learning_rate,
        weight_decay=float(config["weight_decay"]),
    )
    
    # Gradient Clipping ì¶”ê°€ (overfitting ë°©ì§€) - ì´ ë¶€ë¶„ì€ train_ed/vae.pyë¡œ ì´ë™
    # max_grad_norm = 1.0
    # í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ê²°ì„ ìœ„í•œ ê°€ì¤‘ì¹˜ ì„¤ì • (Real:Fake â‰ˆ 1:5.27)
    weights = torch.tensor([5.27, 1.0]).to(device)
    criterion = nn.CrossEntropyLoss(weight=weights)
    criterion.to(device)
    mse = nn.MSELoss()

    # ê²€ì¦ ì†ì‹¤ ê¸°ë°˜ìœ¼ë¡œ í•™ìŠµë¥  ë™ì  ì¡°ì • 
    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.1)

    if pretrained_model_filename:
        print(f"ğŸ”„ ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ ë¡œë”©: {pretrained_model_filename}")
        model, optimizer, start_epoch, min_loss = load_pretrained(
            model, optimizer, pretrained_model_filename
        )
    else:
        start_epoch = 0
        min_loss = float('inf')

    model.to(device)
    torch.manual_seed(1)
    train_loss, train_acc, valid_loss, valid_acc = [], [], [], []
    since = time.time()
    
    # Early Stopping ì„¤ì •
    patience = 10  # ê²€ì¦ ì†ì‹¤ì´ ê°œì„ ë˜ì§€ ì•ŠëŠ” ìµœëŒ€ ì—í¬í¬ ìˆ˜
    best_epoch = 0
    no_improve_count = 0
    
    print(f"\ní•™ìŠµ ì„¤ì •:")
    print(f"  ì´ ì—í¬í¬: {num_epochs}")
    print(f"  Early Stopping: {patience} ì—í¬í¬ ë™ì•ˆ ê°œì„  ì—†ìœ¼ë©´ ì¤‘ë‹¨")
    print(f"  ë°°ì¹˜ í¬ê¸°: {batch_size}")
    print(f"  ëª©í‘œ: ê²€ì¦ ì†ì‹¤ ìµœì†Œí™”")
    print("="*60)

    for epoch in range(start_epoch, num_epochs):
        epoch_start_time = time.time()
        
        # í•™ìŠµ í•¨ìˆ˜ import
        if mod == "ed":
            from train.train_ed import train, valid
        else:
            from train.train_vae import train, valid
        
        # í•™ìŠµ
        train_loss, train_acc, epoch_train_loss = train(
            model,
            device,
            dataloaders["train"],
            criterion,
            optimizer,
            epoch,
            train_loss,
            train_acc,
            mse,
        )
        
        # ê²€ì¦
        valid_loss, valid_acc = valid(
            model,
            device,
            dataloaders["valid"],
            criterion,
            epoch,
            valid_loss,
            valid_acc,
            mse,
        )
        
        # ìŠ¤ì¼€ì¤„ëŸ¬ step í˜¸ì¶œ (ReduceLROnPlateauëŠ” validation lossë¥¼ ì¸ìë¡œ ë°›ìŒ)
        scheduler.step(valid_loss[-1])
        
        # ì‹œê°ì  ë¡œê¹… (ê¹”ë”í•œ ë²„ì „)
        epoch_time = time.time() - epoch_start_time
        current_lr = optimizer.param_groups[0]['lr']
        
        # ì§„í–‰ë¥  ë°” ê³„ì‚°
        progress = (epoch + 1) / num_epochs
        bar_length = 30
        filled_length = int(bar_length * progress)
        bar = 'â–ˆ' * filled_length + 'â–‘' * (bar_length - filled_length)
        
        # ì„±ëŠ¥ ê°œì„  í‘œì‹œ
        if epoch > 0:
            loss_improvement = valid_loss[-2] - valid_loss[-1] if len(valid_loss) > 1 else 0
            acc_improvement = valid_acc[-1] - valid_acc[-2] if len(valid_acc) > 1 else 0
            
            loss_arrow = "â†‘" if loss_improvement > 0 else "â†“" if loss_improvement < 0 else "â†’"
            acc_arrow = "â†‘" if acc_improvement > 0 else "â†“" if acc_improvement < 0 else "â†’"
        else:
            loss_arrow = "NEW"
            acc_arrow = "NEW"
        
        print(f"\nEPOCH {epoch+1:2d}/{num_epochs} | {bar} | {progress*100:5.1f}%")
        print(f"ì‹œê°„: {epoch_time:6.2f}ì´ˆ | í•™ìŠµë¥ : {current_lr:.2e}")
        print(f"í•™ìŠµ ì†ì‹¤: {epoch_train_loss:8.4f} | {loss_arrow} ê²€ì¦ ì†ì‹¤: {valid_loss[-1]:8.4f}")
        print(f"ê²€ì¦ ì •í™•ë„: {valid_acc[-1]*100:6.2f}% | {acc_arrow} ê°œì„ : {acc_improvement*100:+.2f}%" if epoch > 0 else f"ê²€ì¦ ì •í™•ë„: {valid_acc[-1]*100:6.2f}% | {acc_arrow}")
        print("-" * 60)
        
        # Early Stopping ë° ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
        if valid_loss[-1] < min_loss:
            min_loss = valid_loss[-1]
            best_epoch = epoch
            no_improve_count = 0
            best_model_path = os.path.join("weight", f"best_genconvit_{mod}.pth")
            torch.save({
                "epoch": epoch,
                "state_dict": model.state_dict(),
                "optimizer": optimizer.state_dict(),
                "min_loss": min_loss,
                "valid_acc": valid_acc[-1],
                "model_type": mod
            }, best_model_path)
            print(f"â˜… ìƒˆë¡œìš´ ìµœê³  ì„±ëŠ¥! ëª¨ë¸ ì €ì¥: {best_model_path}")
        else:
            no_improve_count += 1
            print(f"âš ï¸  {no_improve_count}/{patience} ì—í¬í¬ ë™ì•ˆ ê°œì„  ì—†ìŒ")
            
            # Early Stopping ì²´í¬
            if no_improve_count >= patience:
                print(f"\nğŸ›‘ Early Stopping! {patience} ì—í¬í¬ ë™ì•ˆ ê°œì„  ì—†ì–´ì„œ í•™ìŠµ ì¤‘ë‹¨")
                print(f"â˜… ìµœê³  ì„±ëŠ¥: ì—í¬í¬ {best_epoch}, ì†ì‹¤ {min_loss:.4f}")
                break

    time_elapsed = time.time() - since

    print(f"\n{'='*60}")
    print(f"âœ“ {model_name} í•™ìŠµ ì™„ë£Œ!")
    print(f"{'='*60}")
    print(f"ì´ ì†Œìš” ì‹œê°„: {time_elapsed // 60:.0f}ë¶„ {time_elapsed % 60:.0f}ì´ˆ")
    print(f"ìµœì¢… ê²€ì¦ ì •í™•ë„: {valid_acc[-1]*100:.2f}%")
    print(f"ìµœì¢… ê²€ì¦ ì†ì‹¤: {valid_loss[-1]:.4f}")
    print(f"ìµœê³  ì„±ëŠ¥: ì—í¬í¬ {best_epoch}, ì†ì‹¤ {min_loss:.4f}")
    print(f"{'='*60}")

    # ëª¨ë¸ ì €ì¥
    file_path = os.path.join(
        "weight",
        f'genconvit_{mod}_{time.strftime("%b_%d_%Y_%H_%M_%S", time.localtime())}',
    )

    # í•™ìŠµ íˆìŠ¤í† ë¦¬ ì €ì¥
    with open(f"{file_path}.pkl", "wb") as f:
        pickle.dump([train_loss, train_acc, valid_loss, valid_acc], f)

    # ìµœì¢… ëª¨ë¸ ì €ì¥
    state = {
        "epoch": num_epochs,
        "state_dict": model.state_dict(),
        "optimizer": optimizer.state_dict(),
        "min_loss": valid_loss[-1],
        "final_acc": valid_acc[-1],
        "config": config,
        "model_type": mod
    }

    weight = f"{file_path}.pth"
    torch.save(state, weight)

    print(f"ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {weight}")
    print("="*60)
    
    return weight, valid_acc[-1], valid_loss[-1]


def main():
    parser = argparse.ArgumentParser(description="GenConViT í•™ìŠµ (ë‹¨ì¼/ë³‘ë ¬ ëª¨ë“œ ì§€ì›)")
    parser.add_argument("-d", "--data", required=True, help="í•™ìŠµ ë°ì´í„° ê²½ë¡œ")
    parser.add_argument("-e", "--epochs", type=int, required=True, help="í•™ìŠµ ì—í¬í¬ ìˆ˜")
    parser.add_argument("-b", "--batch_size", type=int, default=32, help="ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ê°’: 32)")
    parser.add_argument("-m", "--model", choices=["ed", "vae", "both"], help="ëª¨ë¸ íƒ€ì… (both: ë³‘ë ¬ í•™ìŠµ)")
    parser.add_argument("-p", "--pretrained", help="ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ íŒŒì¼ (ë‹¨ì¼ ëª¨ë¸ìš©)")
    parser.add_argument("--pretrained-ed", help="ì‚¬ì „ í›ˆë ¨ëœ ED ëª¨ë¸ íŒŒì¼ (ë³‘ë ¬ í•™ìŠµìš©)")
    parser.add_argument("--pretrained-vae", help="ì‚¬ì „ í›ˆë ¨ëœ VAE ëª¨ë¸ íŒŒì¼ (ë³‘ë ¬ í•™ìŠµìš©)")
    parser.add_argument("--parallel", action="store_true", help="ë³‘ë ¬ í•™ìŠµ ëª¨ë“œ ê°•ì œ í™œì„±í™”")
    
    args = parser.parse_args()
    
    # ë³‘ë ¬ í•™ìŠµ ëª¨ë“œ ê²°ì •
    if args.model == "both" or args.parallel:
        # ë³‘ë ¬ í•™ìŠµ ëª¨ë“œ
        print(f"\n{'='*80}")
        print("ğŸš€ GenConViT ë³‘ë ¬ í•™ìŠµ ëª¨ë“œ!")
        print(f"{'='*80}")
        
        # ë³‘ë ¬ í•™ìŠµ ì‹¤í–‰
        results = train_parallel_models(
            data_path=args.data,
            num_epochs=args.epochs,
            batch_size=args.batch_size,
            pretrained_ed=args.pretrained_ed,
            pretrained_vae=args.pretrained_vae
        )
        
    else:
        # ë‹¨ì¼ ëª¨ë¸ í•™ìŠµ ëª¨ë“œ
        if not args.model:
            print("âŒ ì˜¤ë¥˜: ë‹¨ì¼ ëª¨ë¸ í•™ìŠµ ì‹œ --model ì˜µì…˜ì„ ì§€ì •í•´ì£¼ì„¸ìš” (ed ë˜ëŠ” vae)")
            return
            
        print(f"\n{'='*60}")
        print("GenConViT ë‹¨ì¼ ëª¨ë¸ í•™ìŠµ!")
        print(f"{'='*60}")
        print(f"ë°ì´í„° ê²½ë¡œ: {args.data}")
        print(f"ì—í¬í¬ ìˆ˜: {args.epochs}")
        print(f"ë°°ì¹˜ í¬ê¸°: {args.batch_size}")
        print(f"ëª¨ë¸ íƒ€ì…: {args.model}")
        if args.pretrained:
            print(f"ì‚¬ì „ í›ˆë ¨ ëª¨ë¸: {args.pretrained}")
        print(f"{'='*60}")
        
        start_time = perf_counter()
        
        # ë°ì´í„° ë¡œë”©
        print("ë°ì´í„° ë¡œë”© ì¤‘...")
        dataloaders, dataset_sizes = load_data(args.data, args.batch_size)
        print("ë°ì´í„° ë¡œë”© ì™„ë£Œ!")
        
        print(f"ë°ì´í„°ì…‹ í¬ê¸°:")
        for split, size in dataset_sizes.items():
            print(f"  {split}: {size:,}ê°œ")

        # ëª¨ë¸ ìƒì„±
        if args.model == "ed":
            model = GenConViTED(config)
            model_name = "ED (Autoencoder)"
        else:
            model = GenConViTVAE(config)
            model_name = "VAE (Variational Autoencoder)"
        
        # í•™ìŠµ ì‹¤í–‰
        weight, acc, loss = train_simple_model(
            model, args.model, dataloaders, args.epochs, args.pretrained, args.batch_size, model_name
        )
        
        end_time = perf_counter()
        
        print(f"\n{'='*60}")
        print("ì „ì²´ í•™ìŠµ ì™„ë£Œ!")
        print(f"{'='*60}")
        print(f"ì „ì²´ ì‹¤í–‰ ì‹œê°„: {(end_time - start_time) / 60:.1f}ë¶„")
        print(f"ìµœì¢… ê²°ê³¼: ì •í™•ë„ {acc*100:.2f}%, ì†ì‹¤ {loss:.4f}")
        print(f"{'='*60}")


if __name__ == "__main__":
    # ì‚¬ìš©ë²• ì˜ˆì œ ì¶œë ¥
        main()
