import os
import numpy as np
import cv2
import torch
from torchvision import transforms
from tqdm import tqdm
from dataset.loader import normalize_data
from model.config import load_config
from model.genconvit import GenConViT
from decord import VideoReader, cpu
import glob
from PIL import Image, ImageDraw, ImageFont
import mediapipe as mp
import imageio
# GPU ì„¤ì • ê°œì„  - RTX 5090 ìµœì í™” + CUDA í˜¸í™˜ì„± ì²´í¬
def get_device():
    if torch.cuda.is_available():
        try:
            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            torch.cuda.empty_cache()
            device = torch.device("cuda:0")
            
            # CUDA í˜¸í™˜ì„± ì²´í¬
            print("ğŸ” CUDA í˜¸í™˜ì„± ì²´í¬ ì¤‘...")
            print(f"PyTorch CUDA: {torch.version.cuda}")
            print(f"PyTorch Version: {torch.__version__}")
            
            # ê°„ë‹¨í•œ CUDA ì—°ì‚° í…ŒìŠ¤íŠ¸
            test_tensor = torch.randn(1, 3, 224, 224).cuda()
            test_result = torch.nn.functional.relu(test_tensor)
            print("âœ… CUDA ì—°ì‚° í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
            del test_tensor, test_result
            torch.cuda.empty_cache()
            
            # RTX 5090 ìµœì í™” ì„¤ì •
            if "RTX 5090" in torch.cuda.get_device_name(0):
                print("ğŸš€ RTX 5090 ê°ì§€! ìµœì í™” ì„¤ì • ì ìš© ì¤‘...")
                # CUDA ê·¸ë˜í”„ ìµœì í™” í™œì„±í™”
                torch.backends.cuda.enable_math_sdp(True)
                torch.backends.cuda.enable_flash_sdp(True)
                torch.backends.cuda.enable_mem_efficient_sdp(True)
                # ë©”ëª¨ë¦¬ í• ë‹¹ ìµœì í™”
                torch.cuda.set_per_process_memory_fraction(0.95)  # GPU ë©”ëª¨ë¦¬ì˜ 95% ì‚¬ìš©
            
            print(f"Using GPU: {torch.cuda.get_device_name(0)}")
            print(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
            print(f"CUDA Version: {torch.version.cuda}")
            
            # GPU ì„±ëŠ¥ ì •ë³´ ì¶œë ¥
            props = torch.cuda.get_device_properties(0)
            print(f"Compute Capability: {props.major}.{props.minor}")
            print(f"Multi-Processor Count: {props.multi_processor_count}")
            
        except Exception as e:
            print(f"âŒ CUDA ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            print("ğŸ”„ CPU ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
            device = torch.device("cpu")
            
    else:
        device = torch.device("cpu")
        print("Using CPU")
    return device

device = get_device()


def load_genconvit(config, net, ed_weight, vae_weight, fp16):    
    try:
        model = GenConViT(
            config,
            ed= ed_weight,
            vae= vae_weight, 
            net=net,
            fp16=fp16
        )

        # ì „ì—­ device ë³€ìˆ˜ ì‚¬ìš©
        global device
        
        # CUDA ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸ í›„ ëª¨ë¸ ì´ë™
        if torch.cuda.is_available() and device.type == "cuda":
            try:
                model.to(device)
                print("âœ… ëª¨ë¸ì„ GPUë¡œ ì´ë™ ì„±ê³µ!")
            except Exception as e:
                print(f"âŒ GPU ì´ë™ ì‹¤íŒ¨: {e}")
                print("ğŸ”„ CPU ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
                device = torch.device("cpu")
                model.to(device)
        else:
            model.to(device)
        
        model.eval()
        
        # RTX 5090 ìµœì í™”: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
        if torch.cuda.is_available() and device.type == "cuda":
            allocated = torch.cuda.memory_allocated(0) / 1024**2
            reserved = torch.cuda.memory_reserved(0) / 1024**2
            total = torch.cuda.get_device_properties(0).total_memory / 1024**2
            
            print(f"ğŸš€ Model loaded on RTX 5090!")
            print(f"ğŸ“Š Memory Usage:")
            print(f"   - Allocated: {allocated:.1f} MB")
            print(f"   - Reserved: {reserved:.1f} MB")
            print(f"   - Total GPU: {total:.1f} MB")
            print(f"   - Usage: {allocated/total*100:.1f}%")
            
            # RTX 5090ì—ì„œ fp16 ì‚¬ìš© ê¶Œì¥
            if fp16 and "RTX 5090" in torch.cuda.get_device_name(0):
                print("âš¡ RTX 5090ì—ì„œ fp16 ëª¨ë“œ í™œì„±í™” - ì„±ëŠ¥ í–¥ìƒ!")
        else:
            print(f"ğŸ–¥ï¸ Model loaded on {device}")
        
        if fp16 and device.type == "cuda":
            model.half()

        return model
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        raise e


# MediaPipeìœ¼ë¡œ ëŒ€ì²´
def face_rec(frames, p=None, klass=None):
    temp_face = np.zeros((len(frames), 224, 224, 3), dtype=np.uint8)
    count = 0
    
    # MediaPipe ì–¼êµ´ ê²€ì¶œ ì´ˆê¸°í™”
    mp_face_detection = mp.solutions.face_detection
    with mp_face_detection.FaceDetection(
        model_selection=1, 
        min_detection_confidence=0.5
    ) as face_detection:
        
        for _, frame in tqdm(enumerate(frames), total=len(frames)):
            # MediaPipeë¡œ ì–¼êµ´ ê²€ì¶œ
            results = face_detection.process(frame)
            
            if results.detections:
                for detection in results.detections:
                    if count < len(frames):
                        # ì–¼êµ´ ì˜ì—­ ì¶”ì¶œ
                        bbox = detection.location_data.relative_bounding_box
                        h, w, _ = frame.shape
                        x, y, width, height = int(bbox.xmin * w), int(bbox.ymin * h), \
                                            int(bbox.width * w), int(bbox.height * h)
                        
                        face_image = frame[y:y+height, x:x+width]
                        face_image = cv2.resize(
                            face_image, (224, 224), interpolation=cv2.INTER_AREA
                        )
                        
                        temp_face[count] = face_image
                        count += 1
                    else:
                        break

    return ([], 0) if count == 0 else (temp_face[:count], count)

def face_rec_with_original(frames, p=None, klass=None):
    """ì›ë³¸ í”„ë ˆì„ê³¼ ì–¼êµ´ ë°”ìš´ë”© ë°•ìŠ¤ ì •ë³´ë¥¼ í•¨ê»˜ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜"""
    # ìš”ì²­ëœ í”„ë ˆì„ ìˆ˜ë§Œí¼ë§Œ ì •í™•íˆ í• ë‹¹ (ì´ì „ ë°ì´í„° ê°„ì„­ ë°©ì§€)
    max_frames = len(frames)
    temp_face = np.zeros((max_frames, 224, 224, 3), dtype=np.uint8)
    original_frames = []  # ì›ë³¸ í”„ë ˆì„ ì €ì¥ (ìƒˆë¡œ ì´ˆê¸°í™”)
    face_bboxes = []  # ì–¼êµ´ ë°”ìš´ë”© ë°•ìŠ¤ ì €ì¥ (ìƒˆë¡œ ì´ˆê¸°í™”)
    count = 0
    
    # MediaPipe ì–¼êµ´ ê²€ì¶œ ì´ˆê¸°í™”
    mp_face_detection = mp.solutions.face_detection
    with mp_face_detection.FaceDetection(
        model_selection=1, 
        min_detection_confidence=0.5
    ) as face_detection:
        
        for frame_idx, frame in tqdm(enumerate(frames), total=len(frames)):
            # ìµœëŒ€ í”„ë ˆì„ ìˆ˜ ì œí•œ (ëª…í™•í•œ ê²½ê³„)
            if count >= max_frames:
                print(f"ğŸ“ ìµœëŒ€ í”„ë ˆì„ ìˆ˜ ({max_frames})ì— ë„ë‹¬í•˜ì—¬ ì²˜ë¦¬ ì¤‘ë‹¨")
                break
                
            # MediaPipeë¡œ ì–¼êµ´ ê²€ì¶œ
            results = face_detection.process(frame)
            
            if results.detections:
                for detection in results.detections:
                    # ì´ì¤‘ ì²´í¬: countì™€ max_frames ëª¨ë‘ í™•ì¸
                    if count < max_frames and count < len(temp_face):
                        # ì–¼êµ´ ì˜ì—­ ì¶”ì¶œ
                        bbox = detection.location_data.relative_bounding_box
                        h, w, _ = frame.shape
                        x, y, width, height = int(bbox.xmin * w), int(bbox.ymin * h), \
                                            int(bbox.width * w), int(bbox.height * h)
                        
                        # ëª¨ë¸ ì…ë ¥ìš©: 224x224ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
                        face_image = frame[y:y+height, x:x+width]
                        face_image_resized = cv2.resize(
                            face_image, (224, 224), interpolation=cv2.INTER_AREA
                        )
                        
                        temp_face[count] = face_image_resized
                        
                        # ì›ë³¸ í”„ë ˆì„ê³¼ ì–¼êµ´ ë°”ìš´ë”© ë°•ìŠ¤ ì €ì¥
                        original_frames.append(frame.copy())  # ëª…ì‹œì  ë³µì‚¬
                        face_bboxes.append((x, y, width, height))
                        # print(f"í”„ë ˆì„ {count+1}: ì–¼êµ´ ê²€ì¶œ - ë°”ìš´ë”© ë°•ìŠ¤: ({x}, {y}, {width}, {height})")
                        count += 1
                        break  # ì²« ë²ˆì§¸ ì–¼êµ´ë§Œ ì‚¬ìš©
                    else:
                        break

    return ([], 0, [], []) if count == 0 else (temp_face[:count], count, original_frames, face_bboxes)

def preprocess_frame_with_original(frame, original_frames, face_bboxes):
    """ì›ë³¸ í”„ë ˆì„ ì •ë³´ë¥¼ í¬í•¨í•œ ì „ì²˜ë¦¬ í•¨ìˆ˜"""
    # ëª¨ë¸ ì…ë ¥ìš© í…ì„œ (224x224)
    df_tensor = torch.tensor(frame).float()
    df_tensor = df_tensor.permute((0, 3, 1, 2))

    for i in range(len(df_tensor)):
        df_tensor[i] = normalize_data()["vid"](df_tensor[i] / 255.0)
    
    # GPUë¡œ ì´ë™
    df_tensor = df_tensor.to(device)
    
    # ì›ë³¸ í”„ë ˆì„ê³¼ ë°”ìš´ë”© ë°•ìŠ¤ ì •ë³´ë„ í•¨ê»˜ ë°˜í™˜
    return {
        'tensor': df_tensor,
        'original_frames': original_frames,
        'face_bboxes': face_bboxes
    }

# GradCAM hookìš© ì €ì¥ ê³µê°„
gradcam_features = []
gradcam_gradients = []

def gradcam_forward_hook(module, input, output):
    """GradCAMì„ ìœ„í•œ forward hook"""
    gradcam_features.append(output)

def gradcam_backward_hook(module, grad_input, grad_output):
    """GradCAMì„ ìœ„í•œ backward hook"""
    gradcam_gradients.append(grad_output[0])

def generate_gradcam_full_frame(model, original_frame, target_class=None):
    """ì „ì²´ í”„ë ˆì„ ê¸°ë°˜ GradCAM ìƒì„± - ì˜¬ë°”ë¥¸ ì ‘ê·¼ë²•"""
    try:
        # ì „ì²´ í”„ë ˆì„ì„ ëª¨ë¸ ì…ë ¥ í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
        frame_resized = cv2.resize(original_frame, (224, 224), interpolation=cv2.INTER_AREA)
        
        # í…ì„œë¡œ ë³€í™˜ ë° ì •ê·œí™”
        frame_tensor = torch.tensor(frame_resized).float().permute(2, 0, 1).unsqueeze(0)
        frame_tensor = normalize_data()["vid"](frame_tensor / 255.0)
        frame_tensor = frame_tensor.to(device)
        
        # print(f"GradCAM: ì „ì²´ í”„ë ˆì„ ì…ë ¥ í˜•íƒœ: {frame_tensor.shape}")
        
        # ê¸°ì¡´ GradCAM í•¨ìˆ˜ í˜¸ì¶œ
        return generate_gradcam_with_hooks_improved(model, frame_tensor, target_class)
        
    except Exception as e:
        print(f"ì „ì²´ í”„ë ˆì„ GradCAM ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
        return None

def generate_gradcam_with_hooks_improved(model, input_tensor, target_class=None):
    """ê°œì„ ëœ Hook ê¸°ë°˜ GradCAM ìƒì„± í•¨ìˆ˜ - ë¡œì»¬ ë³€ìˆ˜ ì‚¬ìš©"""
    try:
        # GradCAMì„ ìœ„í•´ ëª¨ë¸ì„ train modeë¡œ ì„¤ì • (gradient ê³„ì‚° í™œì„±í™”)
        model.train()
        
        # ë¡œì»¬ ë³€ìˆ˜ë¡œ íŠ¹ì§•ë§µê³¼ gradient ì €ì¥ (í”„ë ˆì„ ê°„ ê°„ì„­ ë°©ì§€)
        local_features = []
        local_gradients = []
        
        # ë¡œì»¬ Hook í•¨ìˆ˜ ì •ì˜
        def local_forward_hook(module, input, output):
            local_features.append(output)
        
        def local_backward_hook(module, grad_input, grad_output):
            local_gradients.append(grad_output[0])
        
        # ì ì ˆí•œ Target Layer ì°¾ê¸°
        target_layer, layer_name = find_optimal_target_layer_for_gradcam(model)
        
        if target_layer is None:
            print("GradCAM: ì ì ˆí•œ Target Layerë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return None
        
        # print(f"GradCAM: {layer_name} ë ˆì´ì–´ì— Hook ë“±ë¡")
        
        # Target layerê°€ gradientë¥¼ ê³„ì‚°í•˜ë„ë¡ ì„¤ì •
        for param in target_layer.parameters():
            param.requires_grad_(True)
        
        # Hook ë“±ë¡ (ë¡œì»¬ í•¨ìˆ˜ ì‚¬ìš©)
        forward_handle = target_layer.register_forward_hook(local_forward_hook)
        backward_handle = target_layer.register_backward_hook(local_backward_hook)
        
        try:
            # Gradient ê³„ì‚°ì„ ìœ„í•œ ì„¤ì •
            input_tensor.requires_grad_(True)
            
            # ëª¨ë¸ì˜ ëª¨ë“  íŒŒë¼ë¯¸í„°ê°€ gradientë¥¼ ê³„ì‚°í•˜ë„ë¡ ì„¤ì •
            for param in model.parameters():
                param.requires_grad_(True)
            
            # ì´ì „ gradient ì™„ì „ ì œê±°
            model.zero_grad()
            torch.cuda.empty_cache() if torch.cuda.is_available() else None
            
            # ëª¨ë¸ ì˜ˆì¸¡
            output = model(input_tensor)
            
            # GenConViT ëª¨ë¸ ì¶œë ¥ ì²˜ë¦¬ (íŠœí”Œ í˜•íƒœ ê³ ë ¤)
            if isinstance(output, tuple):
                output = output[0]  # ë¶„ë¥˜ ê²°ê³¼ë§Œ ì‚¬ìš©
                # print(f"GradCAM: GenConViT íŠœí”Œ ì¶œë ¥ ê°ì§€, ë¶„ë¥˜ ê²°ê³¼ ì‚¬ìš©")
            
            # print(f"GradCAM: ëª¨ë¸ ì¶œë ¥ í˜•íƒœ = {output.shape}")
            # print(f"GradCAM: ëª¨ë¸ ì¶œë ¥ ê°’ = {output.detach().cpu().numpy()}")
            
            # ë°°ì¹˜ ì²˜ë¦¬: ì²« ë²ˆì§¸ í”„ë ˆì„ë§Œ ì‚¬ìš©
            if output.size(0) > 1:
                # print(f"GradCAM: ë°°ì¹˜ í¬ê¸° {output.size(0)} ê°ì§€, ì²« ë²ˆì§¸ í”„ë ˆì„ë§Œ ì‚¬ìš©")
                output = output[0:1]  # ì²« ë²ˆì§¸ í”„ë ˆì„ë§Œ ìœ ì§€
                # print(f"GradCAM: ìˆ˜ì •ëœ ì¶œë ¥ í˜•íƒœ = {output.shape}")
                # print(f"GradCAM: ìˆ˜ì •ëœ ì¶œë ¥ ê°’ = {output.detach().cpu().numpy()}")
            
            # íƒ€ê²Ÿ í´ë˜ìŠ¤ ê²°ì •
            if target_class is None:
                target_class = torch.argmax(output, dim=1)
            
            # print(f"GradCAM: íƒ€ê²Ÿ í´ë˜ìŠ¤ = {target_class.item()}")
            
            # Gradient ê³„ì‚°ì„ ìœ„í•´ ì…ë ¥ í…ì„œì˜ gradient í™œì„±í™” í™•ì¸
            # print(f"GradCAM: ì…ë ¥ í…ì„œ requires_grad = {input_tensor.requires_grad}")
            
            # íƒ€ê²Ÿ í´ë˜ìŠ¤ì— ëŒ€í•œ gradient ê³„ì‚°
            model.zero_grad()
            target_output = output[0, target_class.item()]
            # print(f"GradCAM: íƒ€ê²Ÿ ì¶œë ¥ ê°’ = {target_output.item()}")
            # print(f"GradCAM: íƒ€ê²Ÿ ì¶œë ¥ requires_grad = {target_output.requires_grad}")
            
            # Gradient ê³„ì‚°
            target_output.backward(retain_graph=True)
            
            # Hookì„ í†µí•œ íŠ¹ì§• ë§µê³¼ gradient ì¶”ì¶œ
            if len(local_features) == 0 or len(local_gradients) == 0:
                # print("GradCAM: Hookì„ í†µí•œ íŠ¹ì§• ë§µ ë˜ëŠ” gradient íšë“ ì‹¤íŒ¨")
                return None
            
            fmap = local_features[0].detach()
            grads = local_gradients[0].detach()
            
            # print(f"GradCAM: íŠ¹ì§• ë§µ í˜•íƒœ = {fmap.shape}")
            # print(f"GradCAM: gradient í˜•íƒœ = {grads.shape}")
            
            # í‘œì¤€ Grad-CAM ê³„ì‚° ë¡œì§
            # ê°€ì¤‘ì¹˜ ê³„ì‚°: gradientì˜ ê³µê°„ ì°¨ì›ì— ëŒ€í•œ í‰ê· 
            weights = grads.mean(dim=(2, 3), keepdim=True)
            # print(f"GradCAM: ê°€ì¤‘ì¹˜ í˜•íƒœ = {weights.shape}")
            # print(f"GradCAM: ê°€ì¤‘ì¹˜ ë²”ìœ„ = [{weights.min():.6f}, {weights.max():.6f}]")
            
            # CAM ìƒì„±: ê°€ì¤‘ì¹˜ì™€ íŠ¹ì§• ë§µì˜ ê³±ì˜ í•©
            cam = (weights * fmap).sum(dim=1, keepdim=True)
            # print(f"GradCAM: CAM ìƒì„± í›„ í˜•íƒœ = {cam.shape}")
            # print(f"GradCAM: CAM ì›ë³¸ ë²”ìœ„ = [{cam.min():.6f}, {cam.max():.6f}]")
            
            # ReLU ì ìš©
            cam = torch.relu(cam)
            # print(f"GradCAM: ReLU í›„ ë²”ìœ„ = [{cam.min():.6f}, {cam.max():.6f}]")
            
            # ì›ë³¸ í¬ê¸°ë¡œ ë³´ê°„ (bilinear)
            cam = torch.nn.functional.interpolate(cam, size=(224, 224), mode='bilinear', align_corners=False)
            cam = cam.squeeze().cpu().numpy()
            # print(f"GradCAM: ë³´ê°„ í›„ í˜•íƒœ = {cam.shape}")
            
            # ì •ê·œí™” ê°œì„  - ë” ê°•í•œ ëŒ€ë¹„
            cam = np.maximum(cam, 0)
            cam_min, cam_max = cam.min(), cam.max()
            # print(f"GradCAM: ì •ê·œí™” ì „ ë²”ìœ„ = [{cam_min:.6f}, {cam_max:.6f}]")
            
            if cam_max > cam_min:
                cam = (cam - cam_min) / (cam_max - cam_min)
                # ëŒ€ë¹„ ê°•í™”ë¥¼ ìœ„í•œ ê°ë§ˆ ì¡°ì •
                cam = np.power(cam, 0.7)  # ê°ë§ˆ < 1ë¡œ ë°ì€ ì˜ì—­ ê°•ì¡°
            else:
                # print("GradCAM: ê²½ê³  - ëª¨ë“  ê°’ì´ ë™ì¼í•¨!")
                cam = np.zeros_like(cam)
            
            # print(f"GradCAM: ìµœì¢… CAM í˜•íƒœ = {cam.shape}, ë²”ìœ„ = [{cam.min():.6f}, {cam.max():.6f}]")
            # print(f"GradCAM: 0ì´ ì•„ë‹Œ í”½ì…€ ìˆ˜ = {np.count_nonzero(cam)}")
            
            return cam
            
        finally:
            # Hook ì œê±°
            forward_handle.remove()
            backward_handle.remove()
            
            # ë¡œì»¬ ë³€ìˆ˜ ëª…ì‹œì  ì •ë¦¬
            local_features.clear()
            local_gradients.clear()
            
            # Gradient ì •ë¦¬
            model.zero_grad()
            
            # ëª¨ë¸ì„ ë‹¤ì‹œ eval ëª¨ë“œë¡œ ë³µì›
            model.eval()
            
            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            torch.cuda.empty_cache() if torch.cuda.is_available() else None
            
    except Exception as e:
        print(f"ê°œì„ ëœ Hook ê¸°ë°˜ GradCAM ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        print(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
        return None

def find_optimal_target_layer_for_gradcam(model):
    """GradCAMì„ ìœ„í•œ ìµœì  Target Layer ì°¾ê¸° - Backbone ë„¤íŠ¸ì›Œí¬ ìš°ì„ """
    try:
        # print("GradCAM: ìµœì  Target Layer íƒìƒ‰ ì¤‘...")
        
        if hasattr(model, 'model_ed'):
            # print("   ED ëª¨ë¸ ê°ì§€ë¨")
            
            # 1. ìš°ì„ ìˆœìœ„: backbone ë„¤íŠ¸ì›Œí¬ (ì‹¤ì œ ë¶„ë¥˜ì— ì‚¬ìš©ë˜ëŠ” ë„¤íŠ¸ì›Œí¬)
            if hasattr(model.model_ed, 'backbone'):
                backbone = model.model_ed.backbone
                # print(f"   âœ… Backbone ë„¤íŠ¸ì›Œí¬ ë°œê²¬: {type(backbone).__name__}")
                
                # Backboneì˜ íŠ¹ì§• ì¶”ì¶œ ë ˆì´ì–´ ì°¾ê¸°
                if hasattr(backbone, 'stages'):  # ConvNeXt êµ¬ì¡°
                    last_stage = backbone.stages[-1]
                    # print(f"   âœ… Backbone ë§ˆì§€ë§‰ stage ì„ íƒ: {last_stage}")
                    return last_stage, "ED Backbone ë§ˆì§€ë§‰ Stage"
                
                elif hasattr(backbone, 'features'):  # ì¼ë°˜ì ì¸ CNN êµ¬ì¡°
                    features = backbone.features
                    conv_layers = []
                    for i, layer in enumerate(features):
                        if isinstance(layer, torch.nn.Conv2d):
                            conv_layers.append((i, layer, layer.out_channels))
                            # print(f"   Backbone Conv2d ë ˆì´ì–´ {i}: {layer.out_channels}ì±„ë„")
                    
                    if conv_layers:
                        best_idx, best_layer, best_channels = conv_layers[-1]
                        # print(f"   âœ… Backbone ìµœì  ë ˆì´ì–´ ì„ íƒ: ì¸ë±ìŠ¤ {best_idx}, {best_channels}ì±„ë„")
                        return best_layer, f"ED Backbone Conv2d-{best_idx} ({best_channels}ì±„ë„)"
                
                elif hasattr(backbone, 'layer4'):  # ResNet êµ¬ì¡°
                    last_layer = backbone.layer4
                    # print(f"   âœ… Backbone layer4 ì„ íƒ: {last_layer}")
                    return last_layer, "ED Backbone Layer4"
            
            # 2. ë°±ì—…: encoder ë„¤íŠ¸ì›Œí¬ (ê¸°ì¡´ ë°©ì‹)
            if hasattr(model.model_ed, 'encoder') and hasattr(model.model_ed.encoder, 'features'):
                features = model.model_ed.encoder.features
                # print(f"   ë°±ì—…: Encoderì—ì„œ {len(features)}ê°œ ë ˆì´ì–´ ë°œê²¬")
                
                # ëª¨ë“  Conv2d ë ˆì´ì–´ ì¶œë ¥
                conv_layers = []
                for i, layer in enumerate(features):
                    if isinstance(layer, torch.nn.Conv2d):
                        conv_layers.append((i, layer, layer.out_channels))
                        print(f"   Conv2d ë ˆì´ì–´ {i}: {layer.out_channels}ì±„ë„")
                
                if conv_layers:
                    # ê°€ì¥ ë§ˆì§€ë§‰ Conv2d ë ˆì´ì–´ ì‚¬ìš© (ê°€ì¥ ê³ ìˆ˜ì¤€ íŠ¹ì§•)
                    best_idx, best_layer, best_channels = conv_layers[-1]
                    print(f"   âœ… Encoder ìµœì  ë ˆì´ì–´ ì„ íƒ: ì¸ë±ìŠ¤ {best_idx}, {best_channels}ì±„ë„")
                    return best_layer, f"ED Encoder Conv2d-{best_idx} ({best_channels}ì±„ë„)"
                
                # Conv2dê°€ ì—†ìœ¼ë©´ ë§ˆì§€ë§‰ ë ˆì´ì–´
                last_layer = features[-1]
                print(f"   âœ… Encoder ë§ˆì§€ë§‰ ë ˆì´ì–´ ì‚¬ìš©: {last_layer}")
                return last_layer, "ED Encoder ë§ˆì§€ë§‰ ë ˆì´ì–´"
        
        elif hasattr(model, 'model_vae'):
            print("   VAE ëª¨ë¸ ê°ì§€ë¨")
            
            if hasattr(model.model_vae, 'encoder') and hasattr(model.model_vae.encoder, 'features'):
                features = model.model_vae.encoder.features
                print(f"   ì´ {len(features)}ê°œ ë ˆì´ì–´ ë°œê²¬")
                
                # ëª¨ë“  Conv2d ë ˆì´ì–´ ì¶œë ¥
                conv_layers = []
                for i, layer in enumerate(features):
                    if isinstance(layer, torch.nn.Conv2d):
                        conv_layers.append((i, layer, layer.out_channels))
                        print(f"   Conv2d ë ˆì´ì–´ {i}: {layer.out_channels}ì±„ë„")
                
                if conv_layers:
                    # ê°€ì¥ ë§ˆì§€ë§‰ Conv2d ë ˆì´ì–´ ì‚¬ìš©
                    best_idx, best_layer, best_channels = conv_layers[-1]
                    print(f"   âœ… ìµœì  ë ˆì´ì–´ ì„ íƒ: ì¸ë±ìŠ¤ {best_idx}, {best_channels}ì±„ë„")
                    return best_layer, f"VAE Encoder Conv2d-{best_idx} ({best_channels}ì±„ë„)"
                
                # Conv2dê°€ ì—†ìœ¼ë©´ ë§ˆì§€ë§‰ ë ˆì´ì–´
                last_layer = features[-1]
                print(f"   âœ… ë§ˆì§€ë§‰ ë ˆì´ì–´ ì‚¬ìš©: {last_layer}")
                return last_layer, "VAE Encoder ë§ˆì§€ë§‰ ë ˆì´ì–´"
        
        print("   âŒ ì ì ˆí•œ Target Layerë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
        return None, None
        
    except Exception as e:
        print(f"   âŒ Target Layer íƒìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        print(f"   ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
        return None, None

def create_gradcam_visualization_improved(original_frame, gradcam, prediction, confidence, face_bbox=None):
    """ê°œì„ ëœ GradCAM ì‹œê°í™” í•¨ìˆ˜ - ì „ì²´ í”„ë ˆì„ì— ìì—°ìŠ¤ëŸ¬ìš´ ë§¤í•‘"""
    try:
        # ì›ë³¸ í”„ë ˆì„ì„ numpy ë°°ì—´ë¡œ ë³€í™˜
        if isinstance(original_frame, np.ndarray):
            pass  # ì´ë¯¸ numpy ë°°ì—´
        elif isinstance(original_frame, torch.Tensor):
            original_frame = original_frame.cpu().numpy()
        else:
            print(f"GradCAM: ì§€ì›í•˜ì§€ ì•ŠëŠ” í”„ë ˆì„ íƒ€ì…: {type(original_frame)}")
            return None
        
        # ì›ë³¸ í”„ë ˆì„ ì •ê·œí™” (0-255)
        if original_frame.max() <= 1.0:
            original_frame = (original_frame * 255).astype(np.uint8)
        
        # ì›ë³¸ í”„ë ˆì„ í¬ê¸° í™•ì¸
        original_h, original_w = original_frame.shape[:2]
        # print(f"GradCAM: ì›ë³¸ í”„ë ˆì„ í¬ê¸°: {original_h}x{original_w}")
        # print(f"GradCAM: ì…ë ¥ GradCAM í¬ê¸°: {gradcam.shape}")
        
        # GradCAM ì •ê·œí™” - ê°œì„ ëœ ëŒ€ë¹„
        cam = np.maximum(gradcam, 0)
        cam_min, cam_max = cam.min(), cam.max()
        # print(f"GradCAM ì‹œê°í™”: ì…ë ¥ ë²”ìœ„ = [{cam_min:.6f}, {cam_max:.6f}]")
        
        if cam_max > cam_min:
            cam = (cam - cam_min) / (cam_max - cam_min)
            # ëŒ€ë¹„ ê°•í™” - ê°ë§ˆ ì¡°ì •ìœ¼ë¡œ íŠ¹ì§• ê°•ì¡°
            cam = np.power(cam, 0.5)  # ê°ë§ˆ < 1ë¡œ ë°ì€ ì˜ì—­ ë” ê°•ì¡°
        
        cam = np.uint8(255 * cam)
        # print(f"GradCAM ì‹œê°í™”: ì •ê·œí™” í›„ ë²”ìœ„ = [{cam.min()}, {cam.max()}]")
        # print(f"GradCAM ì‹œê°í™”: 0ì´ ì•„ë‹Œ í”½ì…€ ìˆ˜ = {np.count_nonzero(cam)}")
        
        # ì „ì²´ í”„ë ˆì„ìœ¼ë¡œ GradCAM í™•ì¥ (ìì—°ìŠ¤ëŸ¬ìš´ ë§¤í•‘)
        # print(f"GradCAM: ì „ì²´ í”„ë ˆì„ìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ë§¤í•‘ - {cam.shape} -> ({original_h}, {original_w})")
        cam_resized = cv2.resize(cam, (original_w, original_h), interpolation=cv2.INTER_CUBIC)
        
        # íˆíŠ¸ë§µ ìƒì„± (JET ì»¬ëŸ¬ë§µ)
        heatmap = cv2.applyColorMap(cam_resized, cv2.COLORMAP_JET)
        heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)
        
        # ì›ë³¸ê³¼ íˆíŠ¸ë§µ í•©ì„± - íˆíŠ¸ë§µ ê°•ì¡°
        alpha = 0.6  # íˆíŠ¸ë§µ ë¹„ì¤‘ ë†’ì„ (ì›ë³¸ 40%, íˆíŠ¸ë§µ 60%)
        overlay = (1 - alpha) * original_frame + alpha * heatmap
        overlay = np.uint8(np.clip(overlay, 0, 255))
        # print(f"GradCAM ì‹œê°í™”: í•©ì„± ë¹„ìœ¨ = ì›ë³¸({1-alpha:.1f}) + íˆíŠ¸ë§µ({alpha:.1f})")
        
        # ì˜ˆì¸¡ ê²°ê³¼ í…ìŠ¤íŠ¸ ì¶”ê°€
        prediction_text = f"{prediction}: {confidence:.3f} (Full-Frame)"
        
        # PIL Imageë¡œ ë³€í™˜í•˜ì—¬ í…ìŠ¤íŠ¸ ì¶”ê°€
        overlay_pil = Image.fromarray(overlay)
        draw = ImageDraw.Draw(overlay_pil)
        
        try:
            # Windows í°íŠ¸ ê²½ë¡œ ì‹œë„
            font = ImageFont.truetype("arial.ttf", 40)
        except IOError:
            try:
                # Linux í°íŠ¸ ê²½ë¡œ ì‹œë„
                font = ImageFont.truetype("/usr/share/fonts/truetype/liberation/LiberationSans-Regular.ttf", 40)
            except IOError:
                font = ImageFont.load_default()
        
        # í…ìŠ¤íŠ¸ ìœ„ì¹˜ ì„¤ì • (ì¢Œì¸¡ ìƒë‹¨)
        text_position = (10, 10)
        text_color = (255, 255, 255)  # í°ìƒ‰
        
        draw.text(text_position, prediction_text, font=font, fill=text_color)
        
        # numpy ë°°ì—´ë¡œ ë‹¤ì‹œ ë³€í™˜í•˜ì—¬ ë°˜í™˜
        final_visualization = np.array(overlay_pil)
        
        # print(f"GradCAM: ì‹œê°í™” ìƒì„± ì™„ë£Œ - ìµœì¢… í¬ê¸°: {final_visualization.shape}")
        return final_visualization
        
    except Exception as e:
        print(f"ê°œì„ ëœ GradCAM ì‹œê°í™” ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        print(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
        return original_frame

def create_debug_visualization(original_frame, gradcam, face_bbox=None):
    """ë””ë²„ê¹…ì„ ìœ„í•œ ì‹œê°í™” - Face bboxì™€ GradCAM ìœ„ì¹˜ í™•ì¸"""
    try:
        debug_frame = original_frame.copy()
        
        # Face bounding box ê·¸ë¦¬ê¸° (ë…¹ìƒ‰)
        if face_bbox is not None:
            x, y, width, height = face_bbox
            cv2.rectangle(debug_frame, (x, y), (x + width, y + height), (0, 255, 0), 3)
            cv2.putText(debug_frame, f"Face: ({x},{y},{width},{height})", 
                       (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
        
        # GradCAMì˜ ìµœëŒ€ í™œì„±í™” ì§€ì  ì°¾ê¸°
        if gradcam is not None:
            # ì›ë³¸ í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
            gradcam_resized = cv2.resize(gradcam, (debug_frame.shape[1], debug_frame.shape[0]))
            max_y, max_x = np.unravel_index(np.argmax(gradcam_resized), gradcam_resized.shape)
            
            # ìµœëŒ€ í™œì„±í™” ì§€ì  í‘œì‹œ (ë¹¨ê°„ìƒ‰ ì›)
            cv2.circle(debug_frame, (max_x, max_y), 10, (255, 0, 0), -1)
            cv2.putText(debug_frame, f"Max: ({max_x},{max_y})", 
                       (max_x + 15, max_y), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 0), 2)
            
            # print(f"ë””ë²„ê·¸: Face bbox = {face_bbox}")
            # print(f"ë””ë²„ê·¸: GradCAM ìµœëŒ€ í™œì„±í™” ì§€ì  = ({max_x}, {max_y})")
            
            # Face ì¤‘ì‹¬ì ê³¼ GradCAM ìµœëŒ€ì  ê±°ë¦¬ ê³„ì‚°
            if face_bbox is not None:
                face_center_x = x + width // 2
                face_center_y = y + height // 2
                distance = np.sqrt((max_x - face_center_x)**2 + (max_y - face_center_y)**2)
                # print(f"ë””ë²„ê·¸: Face ì¤‘ì‹¬ = ({face_center_x}, {face_center_y})")
                # print(f"ë””ë²„ê·¸: ì¤‘ì‹¬ì ê³¼ì˜ ê±°ë¦¬ = {distance:.1f} í”½ì…€")
                
                # ì¤‘ì‹¬ì ë„ í‘œì‹œ (íŒŒë€ìƒ‰)
                cv2.circle(debug_frame, (face_center_x, face_center_y), 8, (0, 0, 255), -1)
                cv2.putText(debug_frame, "Face Center", 
                           (face_center_x + 15, face_center_y), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)
        
        return debug_frame
        
    except Exception as e:
        print(f"ë””ë²„ê·¸ ì‹œê°í™” ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
        return original_frame

def create_gradcam_gif(gradcam_dir, output_dir, video_name, fps=2, duration=400):
    """GradCAM í”„ë ˆì„ë“¤ì„ GIFë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜"""
    try:
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(output_dir, exist_ok=True)
        
        # GradCAM ì´ë¯¸ì§€ íŒŒì¼ë“¤ ì°¾ê¸°
        gradcam_files = sorted([f for f in os.listdir(gradcam_dir) if f.endswith('_gradcam.jpg')])
        
        if len(gradcam_files) == 0:
            print(f"GIF: {gradcam_dir}ì—ì„œ GradCAM ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        # GradCAM GIF ìƒì„±
        gradcam_images = []
        target_size = None
        
        # ì²« ë²ˆì§¸ ì´ë¯¸ì§€ í¬ê¸°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì„¤ì •
        for file in gradcam_files:
            img_path = os.path.join(gradcam_dir, file)
            img = imageio.imread(img_path)
            
            if target_size is None:
                target_size = (img.shape[1], img.shape[0])  # (width, height)
            
            # ëª¨ë“  ì´ë¯¸ì§€ë¥¼ ë™ì¼í•œ í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
            if img.shape[:2] != target_size[::-1]:  # (height, width) vs (width, height)
                img = cv2.resize(img, target_size, interpolation=cv2.INTER_CUBIC)
            
            gradcam_images.append(img)
        
        gradcam_gif_path = None
        if gradcam_images:
            gradcam_gif_path = os.path.join(output_dir, f"{video_name}_gradcam.gif")
            imageio.mimsave(gradcam_gif_path, gradcam_images, duration=duration)
        
        # í†µí•© GIF ìƒì„± (GradCAM ë‘ ê°œë¥¼ ë‚˜ë€íˆ ë°°ì¹˜)
        combined_gif_path = None
        if gradcam_images:
            combined_images = []
            combined_target_size = None
            
            for gradcam_file in gradcam_files:
                gradcam_img = imageio.imread(os.path.join(gradcam_dir, gradcam_file))
                
                # ê°™ì€ ì´ë¯¸ì§€ë¥¼ ë‘ ë²ˆ ë‚˜ë€íˆ ë°°ì¹˜
                combined = np.hstack([gradcam_img, gradcam_img])
                
                # í†µí•© ì´ë¯¸ì§€ë„ ë™ì¼í•œ í¬ê¸°ë¡œ ë§ì¶”ê¸°
                if combined_target_size is None:
                    combined_target_size = (combined.shape[1], combined.shape[0])  # (width, height)
                
                if combined.shape[:2] != combined_target_size[::-1]:
                    combined = cv2.resize(combined, combined_target_size, interpolation=cv2.INTER_CUBIC)
                
                combined_images.append(combined)
            
            combined_gif_path = os.path.join(output_dir, f"{video_name}_combined.gif")
            imageio.mimsave(combined_gif_path, combined_images, duration=duration)
        
        return {
            'gradcam_gif': gradcam_gif_path,
            'combined_gif': combined_gif_path
        }
        
    except Exception as e:
        print(f"GIF ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
        print(f"ğŸ’¡ ì¼ë¶€ ì´ë¯¸ì§€ì˜ í¬ê¸°ê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë¯¸ì§€ í¬ê¸° í†µì¼ì„ ì‹œë„ ì¤‘...")
        import traceback
        print(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
        return None


def preprocess_frame(frame):
    # CPUì—ì„œ ì „ì²˜ë¦¬ í›„ GPUë¡œ ì´ë™
    df_tensor = torch.tensor(frame).float()
    df_tensor = df_tensor.permute((0, 3, 1, 2))

    for i in range(len(df_tensor)):
        df_tensor[i] = normalize_data()["vid"](df_tensor[i] / 255.0)
    
    # GPUë¡œ ì´ë™
    df_tensor = df_tensor.to(device)
    
    return df_tensor


def pred_vid(df, model):
    with torch.no_grad():
        return max_prediction_value(torch.sigmoid(model(df).squeeze()))


def pred_vid_frame_by_frame(df, model, max_frames=None):
    """í”„ë ˆì„ë³„ ë¡œì§“ì„ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜"""
    with torch.no_grad():
        # ê° í”„ë ˆì„ë³„ë¡œ ì˜ˆì¸¡ ìˆ˜í–‰
        frame_logits = []
        frame_predictions = []
        
        # ì²˜ë¦¬í•  í”„ë ˆì„ ìˆ˜ ê²°ì • (ëª…ì‹œì  ì œí•œ)
        total_frames = len(df)
        process_frames = min(total_frames, max_frames) if max_frames else total_frames
        
        for i in range(process_frames):
            try:
                # ë‹¨ì¼ í”„ë ˆì„ì„ ë°°ì¹˜ ì°¨ì›ìœ¼ë¡œ í™•ì¥
                single_frame = df[i:i+1]
                logit = model(single_frame).squeeze()
                prediction = torch.sigmoid(logit)
                
                # ë¡œì§“ê°’ ì²˜ë¦¬ - ì™„ì „íˆ ì•ˆì „í•œ ë³€í™˜
                if logit.dim() == 0:  # ìŠ¤ì¹¼ë¼ í…ì„œ
                    frame_logits.append(float(logit.cpu().item()))
                else:  # ë²¡í„° í…ì„œ
                    logit_np = logit.cpu().numpy()
                    if logit_np.size == 1:  # ë‹¨ì¼ ìš”ì†Œ ë°°ì—´
                        frame_logits.append(float(logit_np.item()))
                    else:  # ë‹¤ì¤‘ ìš”ì†Œ ë°°ì—´
                        frame_logits.append([float(x) for x in logit_np.flatten()])
                
                # ì˜ˆì¸¡ê°’ ì²˜ë¦¬ - ì™„ì „íˆ ì•ˆì „í•œ ë³€í™˜
                if prediction.dim() == 0:  # ìŠ¤ì¹¼ë¼ í…ì„œ
                    frame_predictions.append(float(prediction.cpu().item()))
                else:  # ë²¡í„° í…ì„œ
                    pred_np = prediction.cpu().numpy()
                    if pred_np.size == 1:  # ë‹¨ì¼ ìš”ì†Œ ë°°ì—´
                        frame_predictions.append(float(pred_np.item()))
                    else:  # ë‹¤ì¤‘ ìš”ì†Œ ë°°ì—´
                        frame_predictions.append([float(x) for x in pred_np.flatten()])
                        
            except Exception as e:
                print(f"í”„ë ˆì„ {i+1} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                frame_logits.append(0.0)
                frame_predictions.append(0.5)
        
        # ì „ì²´ í”„ë ˆì„ì— ëŒ€í•œ í‰ê·  ì˜ˆì¸¡ê°’ë„ ê³„ì‚°
        try:
            all_frames = model(df).squeeze()
            overall_prediction = torch.sigmoid(all_frames)
            overall_result = max_prediction_value(overall_prediction)
        except Exception as e:
            print(f"ì „ì²´ í”„ë ˆì„ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            overall_result = (0, 0.5)
        
        return {
            'frame_logits': frame_logits,
            'frame_predictions': frame_predictions,
            'overall_result': overall_result
        }


def max_prediction_value(y_pred):
    # Finds the index and value of the maximum prediction value.
    mean_val = torch.mean(y_pred, dim=0)
    return (
        torch.argmax(mean_val).item(),
        mean_val[0].item()
        if mean_val[0] > mean_val[1]
        else abs(1 - mean_val[1]).item(),
    )


def real_or_fake(prediction):
    return {0: "REAL", 1: "FAKE"}[prediction ^ 1]

def cleanup_gpu_memory():
    """GPU ë©”ëª¨ë¦¬ ì •ë¦¬ - í”„ë ˆì„ ê°„ ê°„ì„­ ë°©ì§€"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
        # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ê°•ì œ ì‹¤í–‰
        import gc
        gc.collect()

# ì˜ìƒ íŒŒì¼ì— ëŒ€í•´ í”„ë ˆì„ì„ ì¶”ì¶œí•œë‹¤.
def extract_frames(video_file, num_frames=15):
    vr = VideoReader(video_file, ctx=cpu(0))
    total_frames = len(vr)

    if num_frames == -1: 
        # if -1, get all frames
        indices = np.arange(total_frames).astype(int)
    else:
        # ìš”ì²­ëœ í”„ë ˆì„ ìˆ˜ë§Œí¼ ì •í™•íˆ ì¶”ì¶œ
        indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)
    
    frames = vr.get_batch(indices).asnumpy()
    
    # ìš”ì²­ëœ í”„ë ˆì„ ìˆ˜ì™€ ì‹¤ì œ ë°˜í™˜ í”„ë ˆì„ ìˆ˜ê°€ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
    if len(frames) != num_frames and num_frames != -1:
        print(f"âš ï¸  í”„ë ˆì„ ìˆ˜ ë¶ˆì¼ì¹˜: ìš”ì²­ {num_frames}, ì‹¤ì œ {len(frames)}")
    
    return frames[:num_frames] if num_frames != -1 else frames  # ì •í™•í•œ ìˆ˜ë§Œ ë°˜í™˜


def df_face_from_folder(vid, num_frames):
    img_list = glob.glob(vid+"/*")
    img = []
    for f in img_list:
        try:
            im = Image.open(f).convert('RGB')
            img.append(np.asarray(im))
        except:
            pass
 
    face, count = face_rec(img[:num_frames])
    return preprocess_frame(face) if count > 0 else []

def df_face_from_image(img_path):
    """ë‹¨ì¼ ì´ë¯¸ì§€ì—ì„œ ì–¼êµ´ì„ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜"""
    try:
        # ì´ë¯¸ì§€ ë¡œë“œ
        im = Image.open(img_path).convert('RGB')
        img_array = np.asarray(im)
        
        # ì–¼êµ´ ê²€ì¶œ
        face, count = face_rec([img_array])
        
        if count > 0:
            return preprocess_frame(face)
        else:
            print(f"âŒ ì´ë¯¸ì§€ì—ì„œ ì–¼êµ´ì„ ê²€ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {img_path}")
            return []
    except Exception as e:
        print(f"âŒ ì´ë¯¸ì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return []

def df_face_from_image_with_original(img_path):
    """ë‹¨ì¼ ì´ë¯¸ì§€ì—ì„œ ì–¼êµ´ì„ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜ (ì›ë³¸ ì •ë³´ í¬í•¨)"""
    try:
        # ì´ë¯¸ì§€ ë¡œë“œ
        im = Image.open(img_path).convert('RGB')
        img_array = np.asarray(im)
        
        # ì–¼êµ´ ê²€ì¶œ (ì›ë³¸ ì •ë³´ í¬í•¨)
        face, count, original_frames, face_bboxes = face_rec_with_original([img_array])
        
        if count > 0:
            processed_tensor = preprocess_frame(face)
            return {
                'tensor': processed_tensor,
                'original_frames': original_frames,
                'face_bboxes': face_bboxes
            }
        else:
            print(f"âŒ ì´ë¯¸ì§€ì—ì„œ ì–¼êµ´ì„ ê²€ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {img_path}")
            return None
    except Exception as e:
        print(f"âŒ ì´ë¯¸ì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

def df_face(vid, num_frames):
    img = extract_frames(vid, num_frames)
    face, count = face_rec(img)
    return preprocess_frame(face) if count > 0 else []

def df_face_with_original(vid, num_frames):
    """ì›ë³¸ í”„ë ˆì„ ì •ë³´ë¥¼ í¬í•¨í•œ ì–¼êµ´ ì¶”ì¶œ í•¨ìˆ˜"""
    print(f"ğŸ¬ ë¹„ë””ì˜¤ì—ì„œ {num_frames}ê°œ í”„ë ˆì„ ì¶”ì¶œ ì‹œì‘...")
    img = extract_frames(vid, num_frames)
    print(f"ğŸ“¹ ì¶”ì¶œëœ í”„ë ˆì„ ìˆ˜: {len(img)}")
    
    face, count, original_frames, face_bboxes = face_rec_with_original(img)
    print(f"ğŸ‘¤ ì–¼êµ´ ê²€ì¶œëœ í”„ë ˆì„ ìˆ˜: {count}")
    
    if count > 0:
        # ìš”ì²­ëœ í”„ë ˆì„ ìˆ˜ì™€ ì‹¤ì œ ê²€ì¶œëœ í”„ë ˆì„ ìˆ˜ í™•ì¸
        if count != num_frames:
            print(f"âš ï¸  í”„ë ˆì„ ìˆ˜ ë¶ˆì¼ì¹˜: ìš”ì²­ {num_frames}, ê²€ì¶œ {count}")
        
        return preprocess_frame_with_original(face, original_frames, face_bboxes)
    else:
        print("âŒ ì–¼êµ´ì´ ê²€ì¶œëœ í”„ë ˆì„ì´ ì—†ìŠµë‹ˆë‹¤.")
        return []


def is_video(vid):
    return os.path.isfile(vid) and vid.endswith(
        tuple([".avi", ".mp4", ".mpg", ".mpeg", ".mov"])
    )

def is_image(img_path):
    """ì´ë¯¸ì§€ íŒŒì¼ì¸ì§€ í™•ì¸í•˜ëŠ” í•¨ìˆ˜"""
    return os.path.isfile(img_path) and img_path.lower().endswith(
        tuple([".jpg", ".jpeg", ".png", ".bmp", ".tiff", ".tif"])
    )

def is_video_folder(vid_folder):
    img_list = glob.glob(vid_folder+"/*")
    return len(img_list)>=1 and img_list[0].endswith(tuple(["png", "jpeg","jpg"]))


def set_result():
    return {
        "video": {
            "name": [],
            "pred": [],
            "klass": [],
            "pred_label": [],
            "correct_label": [],
        }
    }


def store_result(
    result, filename, y, y_val, klass, correct_label=None, compression=None
):
    result["video"]["name"].append(filename)
    result["video"]["pred"].append(y_val)
    result["video"]["klass"].append(klass.lower())
    result["video"]["pred_label"].append(real_or_fake(y))

    if correct_label is not None:
        result["video"]["correct_label"].append(correct_label)

    if compression is not None:
        result["video"]["compression"].append(compression)

    return result

print("pred_func.py ë™ì‘ ì„±ê³µ")